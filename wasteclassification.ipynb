{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cf4cc24-ad17-4b4b-8615-0d1aba73e9a7",
   "metadata": {},
   "source": [
    "# CNN Waste Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a065cf-5eec-468e-8a6f-b1d5304f4dd9",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aaf88d-c0dd-46ba-afe8-215eaf3e3e29",
   "metadata": {},
   "source": [
    "Purpose, goal, context, blah blah blah"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff3f28-f9e7-4150-a661-a8ab9376c39e",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe5e153e-6370-4ef9-899a-cf6d83eaf1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#!pip install torch -- uncomment first time you run\n",
    "import seaborn as sns\n",
    "#!pip install torch -- uncomment first time you run\n",
    "import torch\n",
    "#!pip install torchvision -- uncomment first time you run\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch import nn, optim\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0e00f1a-ec76-4f7e-bdd1-74001cdc013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define set of transformations- convert image to PyTorch tensor, scale pixels from 0-255 to 0.0 - 1.0, normalizes pixels to -1.0 - 1.0\n",
    "# Output is a (3, 256, 256) array --> Color channels 0.0 - 1.0 and the dimensions 256x256\n",
    "transform_scratch = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "225c44d5-914a-4545-947e-93fb4167b2d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 15000\n",
      "Class labels: ['aerosol_cans', 'aluminum_food_cans', 'aluminum_soda_cans', 'cardboard_boxes', 'cardboard_packaging', 'clothing', 'coffee_grounds', 'disposable_plastic_cutlery', 'eggshells', 'food_waste', 'glass_beverage_bottles', 'glass_cosmetic_containers', 'glass_food_jars', 'magazines', 'newspaper', 'office_paper', 'paper_cups', 'plastic_cup_lids', 'plastic_detergent_bottles', 'plastic_food_containers', 'plastic_shopping_bags', 'plastic_soda_bottles', 'plastic_straws', 'plastic_trash_bags', 'plastic_water_bottles', 'shoes', 'steel_food_cans', 'styrofoam_cups', 'styrofoam_food_containers', 'tea_bags']\n"
     ]
    }
   ],
   "source": [
    "# Load in the dataset and apply the transform\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "full_dataset = ImageFolder(root=\"images/\", transform=transform_scratch)\n",
    "print(f\"Total images: {len(full_dataset)}\") # Print number of images\n",
    "print(f\"Class labels: {full_dataset.classes}\") # Print all of the class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d2768-08f0-4c91-987f-ad01e63bc17b",
   "metadata": {},
   "source": [
    "-- **NOTES** --\n",
    "\n",
    "Here we can mess around with different training splits such as \n",
    "- Random split\n",
    "- Evenly divide each sub-category (Stratified)\n",
    "- Evenly divide each sub-category AND default/real world images (Double Stratified)\n",
    "- Train more on default images, test more on real world (Studio Train)\n",
    "\n",
    "The first three are already implemented below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d99b6b4-59b3-4035-9dba-317e63da2ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Train size: 10500\n",
      "Random Validation size: 2250\n",
      "Random Test size: 2250\n",
      "\n",
      "Stratified Train size: 10500\n",
      "Stratified Validation size: 2250\n",
      "Stratified Test size: 2250\n",
      "\n",
      "Double Stratified Train size: 10500\n",
      "Double Stratified Validation size: 2250\n",
      "Double Stratified Test size: 2250\n",
      "\n",
      "DRW Train size: 10500\n",
      "DRW Validation size: 2250\n",
      "DRW Test size: 2250\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Set the split sizes\n",
    "dataset_size = len(full_dataset)\n",
    "train_size = int(0.7 * dataset_size) # TRAIN proportion = 0.7\n",
    "val_size = int(0.15 * dataset_size) # VALIDATION proportion = 0.15\n",
    "test_size = dataset_size - train_size - val_size  # TEST proportion = 0.15\n",
    "\n",
    "##### RANDOMLY split the dataset #####\n",
    "random_train_dataset, random_val_dataset, random_test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print(f\"Random Train size: {len(train_dataset)}\")\n",
    "print(f\"Random Validation size: {len(val_dataset)}\")\n",
    "print(f\"Random Test size: {len(test_dataset)}\\n\")\n",
    "\n",
    "##### STRATIFIED SPLIT (Evenly distribute each category between the train, val, test proportions) #####\n",
    "# Get all targets from the dataset\n",
    "targets = np.array(full_dataset.targets)\n",
    "\n",
    "# 1st split: Separate 70% training data and 30% temp (which will become val + test)\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "train_idx, temp_idx = next(splitter.split(np.zeros(len(targets)), targets))\n",
    "\n",
    "# Use class labels from the temp split to do a second stratified split (val and test)\n",
    "temp_targets = targets[temp_idx]\n",
    "\n",
    "# 2nd split: Split the 30% temp into 15% val and 15% test\n",
    "splitter2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "val_idx, test_idx = next(splitter2.split(np.zeros(len(temp_idx)), temp_targets))\n",
    "\n",
    "# Since val/test indices refer to the temp set, map them back to the full dataset\n",
    "val_idx = temp_idx[val_idx]\n",
    "test_idx = temp_idx[test_idx]\n",
    "\n",
    "# Create PyTorch Subset objects for each split, using the final indices\n",
    "strat_train_dataset = Subset(full_dataset, train_idx)\n",
    "strat_val_dataset = Subset(full_dataset, val_idx)\n",
    "strat_test_dataset = Subset(full_dataset, test_idx)\n",
    "\n",
    "# Print sizes again\n",
    "print(f\"Stratified Train size: {len(strat_train_dataset)}\")\n",
    "print(f\"Stratified Validation size: {len(strat_val_dataset)}\")\n",
    "print(f\"Stratified Test size: {len(strat_test_dataset)}\\n\")\n",
    "\n",
    "##### DOUBLE STRATIFIED SPLIT (Evenly distribute each category and real world vs default between train/val/test) #####\n",
    "# Step 1: Create a list of full file paths for all images\n",
    "all_paths = [full_dataset.samples[i][0] for i in range(len(full_dataset))]\n",
    "\n",
    "# Step 2: Create a combined stratification label for each image: e.g., \"plastic_water_bottles__default\"\n",
    "combined_labels = []\n",
    "for path in all_paths:\n",
    "    # Example path: images/plastic_water_bottles/default/image1.png\n",
    "    parts = path.split(os.sep)\n",
    "    category = parts[-3]  # e.g., plastic_water_bottles\n",
    "    subtype = parts[-2]   # e.g., default or real_world\n",
    "    combined_label = f\"{category}__{subtype}\"\n",
    "    combined_labels.append(combined_label)\n",
    "\n",
    "combined_labels = np.array(combined_labels)\n",
    "\n",
    "# Step 3: First split: 70% train, 30% temp (val + test)\n",
    "split1 = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "train_idx, temp_idx = next(split1.split(np.zeros(len(combined_labels)), combined_labels))\n",
    "\n",
    "# Step 4: Second split: split temp into 50% val, 50% test (i.e., 15% each overall)\n",
    "temp_labels = combined_labels[temp_idx]\n",
    "split2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "val_idx_rel, test_idx_rel = next(split2.split(np.zeros(len(temp_labels)), temp_labels))\n",
    "\n",
    "# Map relative val/test indices back to full dataset indices\n",
    "val_idx = temp_idx[val_idx_rel]\n",
    "test_idx = temp_idx[test_idx_rel]\n",
    "\n",
    "# Step 5: Create PyTorch Subsets\n",
    "double_strat_train_dataset = Subset(full_dataset, train_idx)\n",
    "double_strat_val_dataset = Subset(full_dataset, val_idx)\n",
    "double_strat_test_dataset = Subset(full_dataset, test_idx)\n",
    "\n",
    "# Optional sanity check\n",
    "print(f\"Double Stratified Train size: {len(double_strat_train_dataset)}\")\n",
    "print(f\"Double Stratified Validation size: {len(double_strat_val_dataset)}\")\n",
    "print(f\"Double Stratified Test size: {len(double_strat_test_dataset)}\\n\")\n",
    "\n",
    "##### DEFAULT v REAL WORLD TRAIN SPLIT (All default goes to train, real_world fills in remaining train, then val & test) #####\n",
    "# Map: class index → class name (e.g., 0 → 'plastic_water_bottles')\n",
    "# Map class index → class name (e.g., 0 → 'plastic_water_bottles')\n",
    "idx_to_class = {v: k for k, v in full_dataset.class_to_idx.items()}\n",
    "\n",
    "# Split default and real_world\n",
    "default_indices = []\n",
    "realworld_indices = []\n",
    "\n",
    "for i, (path, class_idx) in enumerate(full_dataset.samples):\n",
    "    subtype = path.split(os.sep)[-2]  # 'default' or 'real_world'\n",
    "    if subtype == \"default\":\n",
    "        default_indices.append(i)\n",
    "    elif subtype == \"real_world\":\n",
    "        realworld_indices.append(i)\n",
    "\n",
    "# Calculate dataset sizes\n",
    "total_size = len(full_dataset)\n",
    "target_train_size = int(0.7 * total_size)\n",
    "target_val_size = int(0.15 * total_size)\n",
    "target_test_size = total_size - target_train_size - target_val_size\n",
    "\n",
    "# Use all default images in training set\n",
    "train_idx = set(default_indices)\n",
    "\n",
    "# How many more real_world images needed for train?\n",
    "remaining_needed = target_train_size - len(train_idx)\n",
    "\n",
    "# Sanity check\n",
    "if remaining_needed < 0:\n",
    "    raise ValueError(\"Too many default images to satisfy 70% train split!\")\n",
    "\n",
    "# Convert to array\n",
    "realworld_indices = np.array(realworld_indices)\n",
    "\n",
    "# First: get required real_world images for training\n",
    "rw_train_idx, rw_temp_idx = train_test_split(\n",
    "    realworld_indices,\n",
    "    train_size=remaining_needed,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Second: split the rest equally into val and test\n",
    "rw_val_idx, rw_test_idx = train_test_split(\n",
    "    rw_temp_idx,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Final index sets\n",
    "train_idx.update(rw_train_idx)\n",
    "val_idx = set(rw_val_idx)\n",
    "test_idx = set(rw_test_idx)\n",
    "\n",
    "# Create subsets\n",
    "drw_train_dataset = Subset(full_dataset, sorted(train_idx))\n",
    "drw_val_dataset = Subset(full_dataset, sorted(val_idx))\n",
    "drw_test_dataset = Subset(full_dataset, sorted(test_idx))\n",
    "\n",
    "# Print final sizes\n",
    "print(f\"DRW Train size: {len(drw_train_dataset)}\")\n",
    "print(f\"DRW Validation size: {len(drw_val_dataset)}\")\n",
    "print(f\"DRW Test size: {len(drw_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a59a0f-5f64-4323-91a5-42ef93f52c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc42c207-d43a-4f76-b73d-9eaae0d92e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
