{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cf4cc24-ad17-4b4b-8615-0d1aba73e9a7",
   "metadata": {},
   "source": [
    "# CNN Waste Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a065cf-5eec-468e-8a6f-b1d5304f4dd9",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aaf88d-c0dd-46ba-afe8-215eaf3e3e29",
   "metadata": {},
   "source": [
    "Purpose, goal, context, blah blah blah"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff3f28-f9e7-4150-a661-a8ab9376c39e",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe5e153e-6370-4ef9-899a-cf6d83eaf1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#!pip install torch -- uncomment first time you run\n",
    "import seaborn as sns\n",
    "#!pip install torch -- uncomment first time you run\n",
    "import torch\n",
    "#!pip install torchvision -- uncomment first time you run\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch import nn, optim\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0e00f1a-ec76-4f7e-bdd1-74001cdc013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define set of transformations- convert image to PyTorch tensor, scale pixels from 0-255 to 0.0 - 1.0, normalizes pixels to -1.0 - 1.0\n",
    "# Output is a (3, 256, 256) array --> Color channels 0.0 - 1.0 and the dimensions 256x256\n",
    "transform_scratch = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "225c44d5-914a-4545-947e-93fb4167b2d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 15000\n",
      "Class labels: ['aerosol_cans', 'aluminum_food_cans', 'aluminum_soda_cans', 'cardboard_boxes', 'cardboard_packaging', 'clothing', 'coffee_grounds', 'disposable_plastic_cutlery', 'eggshells', 'food_waste', 'glass_beverage_bottles', 'glass_cosmetic_containers', 'glass_food_jars', 'magazines', 'newspaper', 'office_paper', 'paper_cups', 'plastic_cup_lids', 'plastic_detergent_bottles', 'plastic_food_containers', 'plastic_shopping_bags', 'plastic_soda_bottles', 'plastic_straws', 'plastic_trash_bags', 'plastic_water_bottles', 'shoes', 'steel_food_cans', 'styrofoam_cups', 'styrofoam_food_containers', 'tea_bags']\n"
     ]
    }
   ],
   "source": [
    "# Load in the dataset and apply the transform\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "full_dataset = ImageFolder(root=\"images/\", transform=transform_scratch)\n",
    "print(f\"Total images: {len(full_dataset)}\") # Print number of images\n",
    "print(f\"Class labels: {full_dataset.classes}\") # Print all of the class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e00bd4-0a62-43fc-915a-7065a3613c9b",
   "metadata": {},
   "source": [
    "## Define Data Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d2768-08f0-4c91-987f-ad01e63bc17b",
   "metadata": {},
   "source": [
    "-- **NOTES** --\n",
    "\n",
    "Here we can mess around with different training splits such as \n",
    "- Random split\n",
    "- Evenly divide each sub-category (Stratified)\n",
    "- Evenly divide each sub-category AND default/real world images (Double Stratified)\n",
    "- Train more on default images, test more on real world (Studio Train)\n",
    "\n",
    "All of these are implemented below. Maybe we start with either random or double stratified but can test out all going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d99b6b4-59b3-4035-9dba-317e63da2ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Train size: 10500\n",
      "Random Validation size: 2250\n",
      "Random Test size: 2250\n",
      "\n",
      "Stratified Train size: 10500\n",
      "Stratified Validation size: 2250\n",
      "Stratified Test size: 2250\n",
      "\n",
      "Double Stratified Train size: 10500\n",
      "Double Stratified Validation size: 2250\n",
      "Double Stratified Test size: 2250\n",
      "\n",
      "DRW Train size: 10500\n",
      "DRW Validation size: 2250\n",
      "DRW Test size: 2250\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Set the split sizes\n",
    "dataset_size = len(full_dataset)\n",
    "train_size = int(0.7 * dataset_size) # TRAIN proportion = 0.7\n",
    "val_size = int(0.15 * dataset_size) # VALIDATION proportion = 0.15\n",
    "test_size = dataset_size - train_size - val_size  # TEST proportion = 0.15\n",
    "\n",
    "##### RANDOMLY split the dataset #####\n",
    "random_train_dataset, random_val_dataset, random_test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print(f\"Random Train size: {len(random_train_dataset)}\")\n",
    "print(f\"Random Validation size: {len(random_val_dataset)}\")\n",
    "print(f\"Random Test size: {len(random_test_dataset)}\\n\")\n",
    "\n",
    "##### STRATIFIED SPLIT (Evenly distribute each category between the train, val, test proportions) #####\n",
    "## Get all targets from the dataset\n",
    "targets = np.array(full_dataset.targets)\n",
    "\n",
    "# 1st split: Separate 70% training data and 30% temp (which will become val + test)\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "train_idx, temp_idx = next(splitter.split(np.zeros(len(targets)), targets))\n",
    "\n",
    "# Use class labels from the temp split to do a second stratified split (val and test)\n",
    "temp_targets = targets[temp_idx]\n",
    "\n",
    "# 2nd split: Split the 30% temp into 15% val and 15% test\n",
    "splitter2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "val_idx, test_idx = next(splitter2.split(np.zeros(len(temp_idx)), temp_targets))\n",
    "\n",
    "# Since val/test indices refer to the temp set, map them back to the full dataset\n",
    "val_idx = temp_idx[val_idx]\n",
    "test_idx = temp_idx[test_idx]\n",
    "\n",
    "# Create PyTorch Subset objects for each split, using the final indices\n",
    "strat_train_dataset = Subset(full_dataset, train_idx)\n",
    "strat_val_dataset = Subset(full_dataset, val_idx)\n",
    "strat_test_dataset = Subset(full_dataset, test_idx)\n",
    "\n",
    "# Print sizes again\n",
    "print(f\"Stratified Train size: {len(strat_train_dataset)}\")\n",
    "print(f\"Stratified Validation size: {len(strat_val_dataset)}\")\n",
    "print(f\"Stratified Test size: {len(strat_test_dataset)}\\n\")\n",
    "\n",
    "##### DOUBLE STRATIFIED SPLIT (Evenly distribute each category and real world vs default between train/val/test) #####\n",
    "# Step 1: Create a list of full file paths for all images\n",
    "all_paths = [full_dataset.samples[i][0] for i in range(len(full_dataset))]\n",
    "\n",
    "# Step 2: Create a combined stratification label for each image: e.g., \"plastic_water_bottles__default\"\n",
    "combined_labels = []\n",
    "for path in all_paths:\n",
    "    # Example path: images/plastic_water_bottles/default/image1.png\n",
    "    parts = path.split(os.sep)\n",
    "    category = parts[-3]  # e.g., plastic_water_bottles\n",
    "    subtype = parts[-2]   # e.g., default or real_world\n",
    "    combined_label = f\"{category}__{subtype}\"\n",
    "    combined_labels.append(combined_label)\n",
    "\n",
    "combined_labels = np.array(combined_labels)\n",
    "\n",
    "# Step 3: First split: 70% train, 30% temp (val + test)\n",
    "split1 = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "train_idx, temp_idx = next(split1.split(np.zeros(len(combined_labels)), combined_labels))\n",
    "\n",
    "# Step 4: Second split: split temp into 50% val, 50% test (i.e., 15% each overall)\n",
    "temp_labels = combined_labels[temp_idx]\n",
    "split2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "val_idx_rel, test_idx_rel = next(split2.split(np.zeros(len(temp_labels)), temp_labels))\n",
    "\n",
    "# Map relative val/test indices back to full dataset indices\n",
    "val_idx = temp_idx[val_idx_rel]\n",
    "test_idx = temp_idx[test_idx_rel]\n",
    "\n",
    "# Step 5: Create PyTorch Subsets\n",
    "double_strat_train_dataset = Subset(full_dataset, train_idx)\n",
    "double_strat_val_dataset = Subset(full_dataset, val_idx)\n",
    "double_strat_test_dataset = Subset(full_dataset, test_idx)\n",
    "\n",
    "# Optional sanity check\n",
    "print(f\"Double Stratified Train size: {len(double_strat_train_dataset)}\")\n",
    "print(f\"Double Stratified Validation size: {len(double_strat_val_dataset)}\")\n",
    "print(f\"Double Stratified Test size: {len(double_strat_test_dataset)}\\n\")\n",
    "\n",
    "##### DEFAULT v REAL WORLD TRAIN SPLIT (All default goes to train, real_world fills in remaining train, then val & test) #####\n",
    "# Map: class index → class name (e.g., 0 → 'plastic_water_bottles')\n",
    "# Map class index → class name (e.g., 0 → 'plastic_water_bottles')\n",
    "idx_to_class = {v: k for k, v in full_dataset.class_to_idx.items()}\n",
    "\n",
    "# Split default and real_world\n",
    "default_indices = []\n",
    "realworld_indices = []\n",
    "\n",
    "for i, (path, class_idx) in enumerate(full_dataset.samples):\n",
    "    subtype = path.split(os.sep)[-2]  # 'default' or 'real_world'\n",
    "    if subtype == \"default\":\n",
    "        default_indices.append(i)\n",
    "    elif subtype == \"real_world\":\n",
    "        realworld_indices.append(i)\n",
    "\n",
    "# Calculate dataset sizes\n",
    "total_size = len(full_dataset)\n",
    "target_train_size = int(0.7 * total_size)\n",
    "target_val_size = int(0.15 * total_size)\n",
    "target_test_size = total_size - target_train_size - target_val_size\n",
    "\n",
    "# Use all default images in training set\n",
    "train_idx = set(default_indices)\n",
    "\n",
    "# How many more real_world images needed for train?\n",
    "remaining_needed = target_train_size - len(train_idx)\n",
    "\n",
    "# Sanity check\n",
    "if remaining_needed < 0:\n",
    "    raise ValueError(\"Too many default images to satisfy 70% train split!\")\n",
    "\n",
    "# Convert to array\n",
    "realworld_indices = np.array(realworld_indices)\n",
    "\n",
    "# First: get required real_world images for training\n",
    "rw_train_idx, rw_temp_idx = train_test_split(\n",
    "    realworld_indices,\n",
    "    train_size=remaining_needed,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Second: split the rest equally into val and test\n",
    "rw_val_idx, rw_test_idx = train_test_split(\n",
    "    rw_temp_idx,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Final index sets\n",
    "train_idx.update(rw_train_idx)\n",
    "val_idx = set(rw_val_idx)\n",
    "test_idx = set(rw_test_idx)\n",
    "\n",
    "# Create subsets\n",
    "drw_train_dataset = Subset(full_dataset, sorted(train_idx))\n",
    "drw_val_dataset = Subset(full_dataset, sorted(val_idx))\n",
    "drw_test_dataset = Subset(full_dataset, sorted(test_idx))\n",
    "\n",
    "# Print final sizes\n",
    "print(f\"DRW Train size: {len(drw_train_dataset)}\")\n",
    "print(f\"DRW Validation size: {len(drw_val_dataset)}\")\n",
    "print(f\"DRW Test size: {len(drw_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca79a390-168f-4ea7-a913-60746ae05d37",
   "metadata": {},
   "source": [
    "### Implement Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2a59a0f-5f64-4323-91a5-42ef93f52c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Use data loader to improve efficiency of loading data during training\n",
    "batch_size = 32  # Can be adjusted\n",
    "\n",
    "# shuffle = True : Ensures all images from a category aren't trained on consecutively\n",
    "train_loader = DataLoader(random_train_dataset, batch_size=batch_size, shuffle=True) \n",
    "val_loader = DataLoader(random_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(random_test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd941d7-4e38-4bf2-a9ec-8ccfd268bea8",
   "metadata": {},
   "source": [
    "## Build CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc42c207-d43a-4f76-b73d-9eaae0d92e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0ec6c64-f3cf-4576-b924-e66f68096c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b586ace8-5409-4c6e-8c47-e5ceb45ed2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# 80-20 split\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "964252b8-8e17-4c1e-8bef-e43c133ed9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74efda11-3c96-4661-8158-26b649ee5296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WasteClassifierCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(WasteClassifierCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "\n",
    "        # Placeholder for fc1 — we'll initialize it after knowing the flatten size\n",
    "        self._flattened_size = None\n",
    "        self.fc1 = None\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # Initialize fc1 if we haven't yet\n",
    "        if self.fc1 is None:\n",
    "            self._flattened_size = x.shape[1]\n",
    "            self.fc1 = nn.Linear(self._flattened_size, 128).to(x.device)\n",
    "\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad5d9acd-26b4-42df-9de4-a05cf0c0dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(full_dataset.classes)\n",
    "model = WasteClassifierCNN(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39dca5ec-6f1b-4ee7-8841-ceffb4a9c759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fea5261-29c8-489b-b9b6-15106961e4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1065.459, Train Accuracy: 20.75%\n",
      "Epoch 2, Loss: 1026.542, Train Accuracy: 23.36%\n",
      "Epoch 3, Loss: 1002.822, Train Accuracy: 24.92%\n",
      "Epoch 4, Loss: 979.822, Train Accuracy: 26.45%\n",
      "Epoch 5, Loss: 963.695, Train Accuracy: 27.52%\n",
      "Epoch 6, Loss: 945.578, Train Accuracy: 28.23%\n",
      "Epoch 7, Loss: 928.535, Train Accuracy: 30.22%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 7\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss:.3f}, Train Accuracy: {train_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d67fe083-d9b7-478f-bdfb-a47282096c97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "              aerosol_cans       0.20      0.09      0.12       110\n",
      "        aluminum_food_cans       0.26      0.16      0.19       116\n",
      "        aluminum_soda_cans       0.29      0.21      0.25       103\n",
      "           cardboard_boxes       0.38      0.56      0.45       113\n",
      "       cardboard_packaging       0.33      0.18      0.23       106\n",
      "                  clothing       0.35      0.30      0.32       105\n",
      "            coffee_grounds       0.52      0.80      0.63        85\n",
      "disposable_plastic_cutlery       0.34      0.17      0.23        94\n",
      "                 eggshells       0.29      0.23      0.26       100\n",
      "                food_waste       0.33      0.62      0.43        99\n",
      "    glass_beverage_bottles       0.34      0.43      0.38       102\n",
      " glass_cosmetic_containers       0.24      0.33      0.28        99\n",
      "           glass_food_jars       0.38      0.37      0.38       100\n",
      "                 magazines       0.60      0.75      0.66        96\n",
      "                 newspaper       0.48      0.43      0.45       101\n",
      "              office_paper       0.47      0.09      0.15        90\n",
      "                paper_cups       0.21      0.08      0.11       106\n",
      "          plastic_cup_lids       0.23      0.19      0.21        95\n",
      " plastic_detergent_bottles       0.31      0.56      0.40        96\n",
      "   plastic_food_containers       0.50      0.34      0.41        88\n",
      "     plastic_shopping_bags       0.46      0.06      0.10       109\n",
      "      plastic_soda_bottles       0.29      0.29      0.29       106\n",
      "            plastic_straws       0.28      0.48      0.36        99\n",
      "        plastic_trash_bags       0.47      0.44      0.45       103\n",
      "     plastic_water_bottles       0.27      0.31      0.29        89\n",
      "                     shoes       0.22      0.28      0.25        88\n",
      "           steel_food_cans       0.35      0.34      0.34        92\n",
      "            styrofoam_cups       0.25      0.65      0.36        97\n",
      " styrofoam_food_containers       0.26      0.33      0.29       111\n",
      "                  tea_bags       0.24      0.06      0.09       102\n",
      "\n",
      "                  accuracy                           0.33      3000\n",
      "                 macro avg       0.34      0.34      0.31      3000\n",
      "              weighted avg       0.34      0.33      0.31      3000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  2  4  2  0  6  2  2  0  1  5  4  1  3  3  0  0  1 10  2  0  5 12  2\n",
      "   5  7  1  7 13  0]\n",
      " [ 2 18  8  1  1  2  2  1  4  9  0  5  6  3  7  0  2  1  3  4  0  0  1  2\n",
      "   1  0 26  2  3  2]\n",
      " [ 3  3 22  0  0  7  1  1  0  1  1  4  3  6  5  1  0  6  8  0  0  2  6  3\n",
      "   1  6  1  9  2  1]\n",
      " [ 1  1  0 63  7  0  3  0 14  6  2  3  1  0  2  1  4  0  1  0  0  1  0  0\n",
      "   0  0  0  0  3  0]\n",
      " [ 1  1  0 54 19  0  1  0  3  9  0  4  1  1  1  0  3  0  1  0  0  2  2  0\n",
      "   0  0  0  0  2  1]\n",
      " [ 1  1  5  2  0 31  5  2  2  5  3  1  2  5  2  1  1  2  2  1  0  7  6  6\n",
      "   4  5  0  1  0  2]\n",
      " [ 0  0  0  1  3  3 68  0  1  4  0  2  0  1  0  0  0  0  0  0  0  0  0  0\n",
      "   0  2  0  0  0  0]\n",
      " [ 5  3  4  0  1  1  0 16  1  4  3  5  3  0  0  0  0  2  7  2  1  0  6  1\n",
      "  14  3  1  6  3  2]\n",
      " [ 2  0  0 12 12  4  9  1 23 12  1  2  0  0  3  0  1  0  0  0  0  1  2  1\n",
      "   1  7  1  0  4  1]\n",
      " [ 0  0  0  2  0  1 10  0  4 61  6  2  1  0  1  0  0  0  0  0  0  5  4  0\n",
      "   0  1  0  1  0  0]\n",
      " [ 1  0  2  0  0  0  1  2  0  4 44  6  3  2  1  0  2  4  3  0  0  5  5  1\n",
      "   2  1  0  7  6  0]\n",
      " [ 0  0  3  5  0  0  2  3  0  2  2 33  3  0  2  0  1  3  5  1  0  1  0  3\n",
      "   3  6  0 15  6  0]\n",
      " [ 0  0  1  0  2  0  4  0  0  6  7  2 37  0  0  0  6  9  0  1  0  1  1  0\n",
      "   2  1  4 10  4  2]\n",
      " [ 0  2  4  2  0  5  1  0  0  2  0  0  1 72  3  0  0  0  0  0  0  0  1  0\n",
      "   0  1  2  0  0  0]\n",
      " [ 1  6  3  2  0  5  1  1  1  5  1  1  1  6 43  1  0  1  1  0  0  1  5  0\n",
      "   3  4  3  2  3  0]\n",
      " [ 1  4  4  0  0  0  4  3  3  5  1  0  2  2  3  8  1  2  3  4  0  3  7  2\n",
      "   5  8  4  5  6  0]\n",
      " [ 1  0  1  3  7  2  1  1  6  2  4 15  3  1  1  0  8  4  5  2  1  1  4  3\n",
      "   4  4  5 11  5  1]\n",
      " [ 1  1  1  1  0  0  0  4  0  0  8  8  9  0  0  0  1 18  8  1  0  1  4  1\n",
      "   1  3  0 20  4  0]\n",
      " [ 0  0  1  0  0  1  0  0  0  0  6  4  2  0  0  0  0  1 54  2  1  9  3  1\n",
      "   1  1  0  8  1  0]\n",
      " [ 0  2  0  0  0  4  0  0  1  2  3  4  2  0  1  0  0  3  2 30  1  1  2  5\n",
      "   3  0  2 10  7  3]\n",
      " [ 1  2  1  0  0  3  0  7  5  3  5  3  0  0  0  1  0  8 12  1  6 12 10  6\n",
      "   6  3  0  8  6  0]\n",
      " [ 1  1  2  0  0  4  0  0  2  2  8  0  0 11  1  0  1  2 12  3  0 31 11  2\n",
      "   4  1  0  7  0  0]\n",
      " [ 1  0  1  1  0  3  0  1  1  5  8  2  0  2  1  0  0  0  5  0  0  5 48  3\n",
      "   2  3  2  2  2  1]\n",
      " [ 1  1  1  0  0  1  0  1  0  2  1  1  2  0  0  1  0  2  5  3  2  0  6 45\n",
      "   7 15  1  3  2  0]\n",
      " [ 5  4  0  0  0  0  0  0  1  0  1  0  2  2  1  0  1  3 15  1  1  4 12  1\n",
      "  28  0  0  5  1  1]\n",
      " [ 4  1  1  6  1  3 12  0  2  5  0  4  0  0  3  1  1  0  1  0  0  2  2  4\n",
      "   2 25  2  2  2  2]\n",
      " [ 4 13  2  0  0  0  3  0  0  8  0  4  6  1  3  1  0  0  0  0  0  2  3  2\n",
      "   1  2 31  1  5  0]\n",
      " [ 1  1  0  1  1  0  0  0  0  1  1  4  0  0  1  0  2  3  3  0  0  3  1  0\n",
      "   2  1  1 63  7  0]\n",
      " [ 0  2  1  3  2  0  1  1  0  1  5  6  2  0  0  0  0  1  5  1  0  0  0  1\n",
      "   0  0  1 41 37  0]\n",
      " [ 2  0  3  4  1  3  1  0  5 19  2  6  4  3  2  1  3  1  2  1  0  2  6  0\n",
      "   3  3  0  9 10  6]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=full_dataset.classes))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e14d066-6cc0-407d-9681-5e418da7acb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 375/375 [05:19<00:00,  1.17it/s, acc=0.2524, loss=2.6979]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 375/375 [05:20<00:00,  1.17it/s, acc=0.5252, loss=1.6999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████▍                               | 55/375 [00:47<04:35,  1.16it/s, acc=0.7244, loss=0.9383]"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# IMPORTS\n",
    "# ----------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------\n",
    "# DEVICE SETUP\n",
    "# ----------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ----------------------\n",
    "# MODEL DEFINITION\n",
    "# ----------------------\n",
    "class WasteClassifierCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(WasteClassifierCNN, self).__init__()\n",
    "        # Convolutional layers with increasing filters\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)     # Output: (B, 32, 128, 128)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)    # Output: (B, 64, 64, 64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)   # Output: (B, 128, 32, 32)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Halves spatial dimensions each time\n",
    "\n",
    "        # Flattened feature map size: 128 channels × 32 × 32\n",
    "        self.flattened_size = 128 * 32 * 32\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # (B, 32, 128, 128)\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # (B, 64, 64, 64)\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # (B, 128, 32, 32)\n",
    "        x = x.view(x.size(0), -1)             # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# ----------------------\n",
    "# MODEL INITIALIZATION\n",
    "# ----------------------\n",
    "num_classes = len(full_dataset.classes)  # Detect based on your dataset\n",
    "model = WasteClassifierCNN(num_classes=num_classes).to(device)\n",
    "\n",
    "# ----------------------\n",
    "# LOSS FUNCTION & OPTIMIZER\n",
    "# ----------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ----------------------\n",
    "# TRAINING LOOP (Keras-style progress output)\n",
    "# ----------------------\n",
    "num_epochs = 5  # Adjust as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")  # Keras-style clean epoch header\n",
    "\n",
    "    # Progress bar on batches\n",
    "    loop = tqdm(train_loader, total=len(train_loader), leave=True, ncols=100)\n",
    "\n",
    "    for images, labels in loop:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Running loss and accuracy\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Update progress bar\n",
    "        loop.set_postfix({\n",
    "            \"acc\": f\"{(correct / total):.4f}\",\n",
    "            \"loss\": f\"{running_loss / (loop.n + 1):.4f}\"\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da89c0c7-d917-46e7-bed8-1c543f9bb721",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
